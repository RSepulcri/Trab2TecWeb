# Projeto Crawler em Python

Este projeto consiste em uma aplicação de web scraping que coleta informações de imóveis disponíveis para aluguel no site Zap Imóveis. A aplicação também utiliza inteligência artificial através da API Gemini para responder perguntas sobre os dados coletados.

## Pré-requisitos

- **Possuir uma conta Google**
- **Acessar o Google Colab**  
  link: [https://colab.google/](https://colab.research.google.com/drive/1huAnkuhvSEa-oIfiFmi2vpJ1yA1Cb2wT?usp=sharing)

## Configurando Inteligencia Artificial GemIA

### Passo 1 - Clique na chave no canto esquerdo

![Passo 1](/src/img/1.png)

### Passo 2 - Clique em adicionar novo segredo

![Passo 2](/src/img/11.png)

### Passo 3 - Insira: nome "API_KEY_GEMAI", valor "AIzaSyDDhRGiIErRIey3NyP09O-IYyu6P9EzETM" e ative a checkbox

### Passo 4 - Play para configurar o simulador de navegador web

![Passo 4](/src/img/2222.png)

### Passo 5 - Play para configurar o simulador de navegador web2

![Passo 4](/src/img/22222.png)

### Passo 6 - Play para atribuir a variável do sistema o endereço web base da aplicação

![Passo 4](/src/img/222222.png)

### Passo 7 - Ative a função responsável por buscar as informações dos imóveis

![Passo 4](/src/img/2222222.png)

### Passo 8 - Ative a função que coleta os dados dos imóveis

![Passo 4](/src/img/22222222.png)

### Passo 9 - Ative a função para printar as informações e baixar o arquivo .csv contendo as informações dos imóveis encontrados

![Passo 4](/src/img/222222222.png)

### Passo 10 [opcional] - Ative a função para a IA responder perguntas referentes aos dados encontrados pela aplicação

![Passo 4](/src/img/2222222222.png)  
![Passo 4](/src/img/22222222222222.png)

## Rodar localmente

### Passo 1 - Clone o repositório no local desejado e acesse-o

![Passo 1](/src/img/3.png)

### Passo 2 - Acesse a branch 'local'

![Passo 2](/src/img/33.png)

### Passo 3 - Crie um ambiente virtual e acesse-o

![Passo 2](/src/img/333.png)

### Passo 4 - Cole o conteúdo de requirements no terminal e execute

![Passo 3](/src/img/3333.png)

### Passo 5 - Execute o crawler e aguarde a criação do arquivo .csv

![Passo 4](/src/img/33333.png)
![Passo 4](/src/img/333333.png)
